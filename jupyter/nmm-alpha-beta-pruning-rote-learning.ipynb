{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## α-β-Pruning mit Rote-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "css = \"\"\n",
    "if os.path.isfile(\"style.html\"):\n",
    "    from IPython.core.display import HTML\n",
    "    with open(\"style.html\", \"r\") as file:\n",
    "        css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nmm-game.ipynb\n",
    "%run ./nmm-artificial-intelligence.ipynb\n",
    "%run ./nmm-heuristic.ipynb\n",
    "%run ./nmm-symmetry.ipynb\n",
    "%run ./nmm-cache-rote-learning.ipynb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Implementierung des *α-β-Pruning* Algorithmus mit *Rote-Learning* wird eine Klasse `AlphaBetaPruningRoteLearning` implementiert, die ebenfalls von der zuvor definierten Klasse `ArtificialIntelligence` erbt. Auch hier wird die Funktion `bestMoves` überschrieben, die später genauer definiert wird, und den Konstruktor `__init__` implementiert. Der Konstruktor besitzt drei optionale Parameter, die den Algorithmus konfigurieren können:\n",
    "\n",
    "* die `limit` Einstellung setzt die maximale Rekursionstiefe;\n",
    "* die `symmetry` Einstellung legt fest, ob alle symmetrischen Spielfelder zu einem Zustand berechnet werden sollen und dann ebenfalls in der Transpositionstabelle abgelegt werden sollen;\n",
    "* durch den `weights` Parameter können die Gewichtungen der zuvor definierten Heuristik bestimmt werden;\n",
    "* durch den `cache` Parameter kann ein Cache übergeben, falls dies nicht der Fall ist wird ein neuer leerer Cache initialisert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaPruningRoteLearning(ArtificialIntelligence):\n",
    "    def __init__(self, limit=3, symmetry=True, weights=None, cache=None, name=None):\n",
    "        \n",
    "        self.name = name\n",
    "        self.cache = cache\n",
    "        if self.cache is None:\n",
    "            self.cache = CacheRoteLearning()\n",
    "        \n",
    "        self.limit = limit\n",
    "        self.symmetry = symmetry\n",
    "        self.weights = weights\n",
    "        if self.weights is None:\n",
    "            self.weights = HeuristicWeights()\n",
    "    \n",
    "    def bestMoves(self, state, player):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Debuggingzwecke wird eine `__repr__` Funktion implementiert, die eine String-Repräsentation aus einer `AlphaBetaPruning` Instanz mit allen Einstelungen erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self: AlphaBetaPruningRoteLearning):\n",
    "    return f\"AlphaBetaPruningRoteLearning(name={self.name}, limit={self.limit}, symmetry={self.symmetry}, weights={self.weights})\"\n",
    "\n",
    "AlphaBetaPruningRoteLearning.__repr__ = __repr__\n",
    "del __repr__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementierung *α-β-Pruning*\n",
    "#### Hilfsfunktion zum Füllen der Transpositionstabelle `writeCache`\n",
    "\n",
    "Die Hilfsfunktion `writeCache` legt den gegebenen Zustand `state` und Spieler `player` mit samt des errechneten Wertes `value` in der Transpositionstabelle (`cache`) ab. Ist zusätzlich die Einstellung `symmetry` aktiviert, werden auch die errechneten Spielfelder in der Transpositionstabelle gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def writeCache(self, state, player, value):\n",
    "    if self.symmetry:\n",
    "        for symmetricState in findSymmetries(state):\n",
    "            self.cache.write(symmetricState, player, value)\n",
    "    else:\n",
    "        self.cache.write(state, player, value)\n",
    "\n",
    "AlphaBetaPruningRoteLearning.writeCache = writeCache\n",
    "del writeCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktion zur Berechnung des Wertes eines Spielzustands `alphaBeta`\n",
    "\n",
    "Die Funktion `alphaBeta` beinhaltet die Implementierung des *α-β-Pruning* verbunden mit dem Rote-Learning.\n",
    "\n",
    "* Wie zuvor beim *Minimax-Algorithmus*, wird der `utility` Wert zurückgegeben, falls das Spiel in dem State `s` beendet (`finished`) ist.\n",
    "* Ebenfalls äquivalent wird der `heuristic` Wert verwendet, sobald das Rekursionslimit (`limit`) erreicht wird. Allerdings kann hier der Wert aus dem Cache geladen werden, falls dieser zur Verfügung steht.\n",
    "* Der eigentliche α-β-Pruning Alogrithmus errechnet rekursiv den Wert eines Zuges. Hierbei wird der erste Wert der nächsten States verwendet, der größer oder gleich der oberen Grenze `beta` ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaBeta(self, state, player, alpha, beta, limit):\n",
    "    if limit == 0:\n",
    "        value = self.cache.read(state, player)\n",
    "        if value:\n",
    "            self.cache_hit += 1\n",
    "            return value\n",
    "        else:\n",
    "            self.cache_miss += 1\n",
    "            return heuristic(state, player, self.weights)\n",
    "\n",
    "    states = nextStates(state, player)\n",
    "    if finished(state, player, ns=states):\n",
    "        return utility(state, player, ns=states)\n",
    "\n",
    "    val = alpha\n",
    "    for ns in states:\n",
    "        val = max(val, -self.alphaBeta(ns, opponent(player), -beta, -alpha, limit-1))\n",
    "        if val >= beta:\n",
    "            return val\n",
    "        alpha = max(val, alpha)\n",
    "    return val\n",
    "\n",
    "AlphaBetaPruningRoteLearning.alphaBeta = alphaBeta\n",
    "del alphaBeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktion zur Auswahl des bestmöglichen Zuges `bestMoves`\n",
    "\n",
    "Die Funktion `bestMoves` berechnet rekursiv den geschätzten Wert aller möglichen Züge für einen Spieler `player` und wählt die Züge mit dem besten Wert aus. Diese rekursive Suche wird solange ausgeführt, bis die maximale Rekursionstiefe erreicht ist. Abschließend werden aus den erhaltenen Zuständen die Zustände mit dem besten Wert ausgewählt und zurückgegeben.\n",
    "\n",
    "Zusätzlich werden Informationen gesammelt, die die genaueren Abläufe im Algorithmus abbilden:\n",
    "* `runtime` ist die Rechendauer der Funktion in Sekunden;\n",
    "* `limit` ist die erreichte maximale Rekursionstiefe;\n",
    "* `visited` ist die Anzahl der besuchten Zustände;\n",
    "* `cache_hit` ist die Anzahl der Berechenungen, die durch die Tanspositionstabelle (`cache`) eingespart werden konnten;\n",
    "* `cache_miss` ist die Anzahl der Berechnungen, die trotz der Transpositionstabelle durchgeführt werden mussten, da kein Eintrag gefunden wurde;\n",
    "* `cache_size` ist die Anzahl der Zustände im Cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestMoves(self, state, player):\n",
    "    # Start clock\n",
    "    start = time.time()\n",
    "    \n",
    "    # Reset counter\n",
    "    self.visited = 0\n",
    "    self.cache_hit = 0\n",
    "    self.cache_miss = 0\n",
    "    \n",
    "    states = nextStates(state, player)\n",
    "    moves = [\n",
    "        (-self.alphaBeta(s, opponent(player), -1, 1, self.limit), s)\n",
    "        for s in states\n",
    "    ]\n",
    "    maximum = max(v for (v, s) in moves)\n",
    "    \n",
    "    self.writeCache(state, player, maximum)\n",
    "    bestMoves = [s for (v, s) in moves if v == maximum]\n",
    "    \n",
    "    end = time.time()\n",
    "    return BestMoves(\n",
    "        bestMoves,\n",
    "        maximum,\n",
    "        # Collect debug information\n",
    "        {   \"runtime\": end - start,\n",
    "            \"limit\": self.limit,\n",
    "            \"visited\": self.visited,\n",
    "            \"cache_hit\": self.cache_hit,\n",
    "            \"cache_miss\": self.cache_miss,   \n",
    "            \"cache_size\": len(self.cache.cache)\n",
    "        }\n",
    "    )\n",
    "\n",
    "AlphaBetaPruningRoteLearning.bestMoves = bestMoves\n",
    "del bestMoves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
